{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../laliga.sqlite\")\n",
    "df = pd.read_sql_query(\"SELECT * from Matches\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to extract the result and the goals of each match similar to the beginning of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(score: str):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    if goals[0]>goals[1]:\n",
    "        return \"1\"\n",
    "    elif goals[1]>goals[0]:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = df['score'].apply(get_result)\n",
    "df['result'] = df['result'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goals(score: str, home_away: int):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    return goals[home_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_goals'] = df['score'].apply(get_goals, args=(0,))\n",
    "df['away_goals'] = df['score'].apply(get_goals, args=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first features come from the matchday standings calculated in the analytical work, exercise 10. From there, we will use the rank of both of the teams and the goals scored and conceded on average up to that point of the season. We will also use the results of the last 5 games that we computed by computing a form number out of them: A win ist worth +1, a tie 0 and a loss -1. These values are then added up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_form(last_5):\n",
    "    if last_5 != \"[]\":\n",
    "        last_5 = list(last_5)\n",
    "        form = 0\n",
    "        for result in last_5:\n",
    "            if result == \"W\":\n",
    "                form += 1\n",
    "            elif result == \"T\":\n",
    "                pass\n",
    "            elif result == \"L\":\n",
    "                form -= 1\n",
    "        return form\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings = pd.read_excel('../reports/MatchdayStandings.xlsx', engine='openpyxl')\n",
    "df_standings['form'] = df_standings['last_5'].map(get_form)\n",
    "df_standings['GF'] /= df_standings['matchday']\n",
    "df_standings['GA'] /= df_standings['matchday']\n",
    "df_standings['matchday'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'home_team', 'form', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df.rename(columns={'rank': 'home_rank', 'form': 'home_form', 'GF': 'home_GF_pg', 'GA': 'home_GA_pg'}, inplace=True)\n",
    "df_standings.rename(columns={'home_team': 'away_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'away_team', 'form', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')\n",
    "df.rename(columns={'rank': 'away_rank', 'form': 'away_form', 'GF': 'away_GF_pg', 'GA': 'away_GA_pg'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to give the model more information about how well the home team was playing at home and vice versa, as a team might have a low overall rank in the standings but is near the top in the home table for example. For this we compute the home and away table similarly to the exercise 10 and extract the rank of the home team in the home table and the rank of the away team in the away table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = df.loc[df['season'] != '2021-2022'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13156/1614524032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mdf_standings_away\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GD'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_standings_away\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GF'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf_standings_away\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mdf_standings_home\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Pts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mdf_standings_home\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mdf_standings_home\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_standings_home\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'home_rank_HT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\parap\\Qsync\\Uni\\Module\\Master\\3_Semester\\Research and Innovation\\01_Introduction to Python\\la-quiniela\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   5284\u001b[0m                 \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5286\u001b[1;33m             indexer = lexsort_indexer(\n\u001b[0m\u001b[0;32m   5287\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5288\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\parap\\Qsync\\Uni\\Module\\Master\\3_Semester\\Research and Innovation\\01_Introduction to Python\\la-quiniela\\.venv\\lib\\site-packages\\pandas\\core\\sorting.py\u001b[0m in \u001b[0;36mlexsort_indexer\u001b[1;34m(keys, orders, na_position, key)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mna_position\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"last\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"first\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\parap\\Qsync\\Uni\\Module\\Master\\3_Semester\\Research and Innovation\\01_Introduction to Python\\la-quiniela\\.venv\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                 \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\parap\\Qsync\\Uni\\Module\\Master\\3_Semester\\Research and Innovation\\01_Introduction to Python\\la-quiniela\\.venv\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         uniques, codes = safe_sort(\n\u001b[0m\u001b[0;32m    683\u001b[0m             \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\parap\\Qsync\\Uni\\Module\\Master\\3_Semester\\Research and Innovation\\01_Introduction to Python\\la-quiniela\\.venv\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36msafe_sort\u001b[1;34m(values, codes, na_sentinel, assume_unique, verify)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m             \u001b[0msorter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2067\u001b[1;33m             \u001b[0mordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2068\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2069\u001b[0m             \u001b[1;31m# try this anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dfs_home_table = []\n",
    "dfs_away_table = []\n",
    "for season in df_past['season'].drop_duplicates():\n",
    "    for division in df_past.loc[(df_past['season']==season), 'division'].drop_duplicates():\n",
    "        df_games = df_past.loc[(df_past['season']==season) & (df_past['division']==division)]\n",
    "        teams = df_games['home_team'].drop_duplicates().rename('team')\n",
    "        init_data = [(season, division, 0, 0, 0, 0, 0) for _ in teams]\n",
    "        df_standings_home = pd.DataFrame(init_data, columns=['season', 'division', 'matchday', 'GF', 'GA', 'GD', 'Pts'], index=teams)\n",
    "        df_standings_away = pd.DataFrame(init_data, columns=['season', 'division', 'matchday', 'GF', 'GA', 'GD', 'Pts'], index=teams)\n",
    "        for matchday in df_games['matchday'].drop_duplicates():\n",
    "            df_standings_home['matchday'] += 1\n",
    "            df_standings_away['matchday'] += 1\n",
    "            df_matchday = df_games.loc[df_games['matchday']==matchday]            \n",
    "            for i in df_matchday.index:\n",
    "                game = df_matchday.loc[i, :]\n",
    "                df_standings_home.loc[game['home_team'], 'GF'] += game['home_goals']\n",
    "                df_standings_home.loc[game['home_team'], 'GA'] += game['away_goals']\n",
    "                df_standings_away.loc[game['away_team'], 'GF'] += game['away_goals']\n",
    "                df_standings_away.loc[game['away_team'], 'GA'] += game['home_goals']                \n",
    "                if game['result'] == '1':                    \n",
    "                    df_standings_home.loc[game['home_team'], 'Pts'] += 3                    \n",
    "                elif game['result'] == '2':                    \n",
    "                    df_standings_away.loc[game['away_team'], 'Pts'] += 3\n",
    "                else:                    \n",
    "                    df_standings_away.loc[game['home_team'], 'Pts'] += 1\n",
    "                    df_standings_away.loc[game['away_team'], 'Pts'] += 1\n",
    "                    \n",
    "            df_standings_home['GD'] = (df_standings_home['GF'] - df_standings_home['GA']).astype(int)\n",
    "            df_standings_away['GD'] = (df_standings_away['GF'] - df_standings_away['GA']).astype(int)\n",
    "\n",
    "            df_standings_home.sort_values(by=['Pts', 'GD', 'GF'], ascending=False, inplace=True)\n",
    "            df_standings_home.reset_index(inplace=True)\n",
    "            df_standings_home.insert(value=np.arange(1, len(df_standings_home)+1), loc=3, column='home_rank_HT')\n",
    "            dfs_home_table.append(df_standings_home[['season', 'division', 'matchday', 'home_rank_HT', 'team']].copy())\n",
    "            df_standings_home.drop(columns=['home_rank_HT'], inplace=True)\n",
    "            df_standings_home.set_index(keys='team', drop=True, inplace=True)\n",
    "\n",
    "            df_standings_away.sort_values(by=['Pts', 'GD', 'GF'], ascending=False, inplace=True)\n",
    "            df_standings_away.reset_index(inplace=True)\n",
    "            df_standings_away.insert(value=np.arange(1, len(df_standings_away)+1), loc=3, column='away_rank_AT')\n",
    "            dfs_away_table.append(df_standings_away[['season', 'division', 'matchday', 'away_rank_AT', 'team']].copy())\n",
    "            df_standings_away.drop(columns=['away_rank_AT'], inplace=True)\n",
    "            df_standings_away.set_index(keys='team', drop=True, inplace=True)\n",
    "\n",
    "all_home = pd.concat(dfs_home_table, ignore_index=True)\n",
    "all_home['matchday'] += 1\n",
    "all_home.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "all_away = pd.concat(dfs_away_table, ignore_index=True)\n",
    "all_away['matchday'] += 1\n",
    "all_away.rename(columns={'team': 'away_team'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(all_home, left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df = df.merge(all_away, left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our last features, we will use the results of the last 3 direct confrontations. To quantify this, we subtract the number of wins for the away team from the number of wins for the home team. (This might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_confronts(row):\n",
    "    a = row['season']\n",
    "    dt = df_past[(df_past.home_team == row.home_team) & (df_past.away_team == row.away_team)& (df_past.season < a)].tail(3)\n",
    "    res = dt[['result']].values.tolist()\n",
    "    a = res.count(['1']) - res.count(['2'])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_conf'] = df.apply(last_confronts, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will use the season as a feature because the statistics of the match outcomes have changed a lot over the years. For example, the home win rate in the first seasons was even higher than the average one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df['season'].str.split('-', n = 1, expand = True)\n",
    "df['season'] = new[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all cells where we have missing values (e.g. the ranks on the first matchday) by zeros, because the model will not be able to predict instances where feature values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df[['home_rank','away_rank','home_form','away_form','home_GF_pg',\n",
    "              'home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']].copy()\n",
    "df_features[df_features.isna()] = 0\n",
    "df_features[['season','matchday','result']] = df[['season','matchday','result']].copy()\n",
    "df_features = df_features[['season','matchday','result','home_rank','away_rank','home_form','away_form','home_GF_pg',\n",
    "              'home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not execute all the feature extraction again when training the model, we save the file in a csv-format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.to_csv('../quiniela/features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most difficult aspects when training the model is the fact that our available dataset is very imbalanced. In the analytical exercises we have seen that over 52% of the games are a win for the home team. While training our first models, we saw that this led them to predict nearly always a home win. This resulted in a respectable accuracy of around 50%, but of course this is not a desired result.\n",
    "\n",
    "In this section we train a few different models and look also on the precision (what percentage of games predicted to be home/tie/away actually were correctly predicted) and recall (what percentage of games that were home/tie/away were correctly predicted) of each of the classes. In the end we want to use a `VotingClassifier` that uses the results of several different classifiers which then \"vote\" to give their final decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble  import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('../quiniela/features.csv', header=0)\n",
    "df_final['result'] = pd.Categorical(df_final['result'])\n",
    "df_final = df_final[df_final['season']<2021]\n",
    "features = ['season','matchday','home_rank','away_rank','home_form','away_form','home_GF_pg',\n",
    "              'home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']\n",
    "X = df_final[features].copy()\n",
    "y = df_final['result'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our features have vastly different scalings, we scale them to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to test the model's behaviour on unseen data after training, we split our dataset in train- and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a k-nearest-neighbours classifier. To determine the best number of parameters, we use a grid search. The `weights=distance` option weighs the vote of the neighbours by their distance to the sample concerned, which improves the robustness to the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 8}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors' : np.arange(5,9)}\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4811816798118168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.74      0.64      7613\n",
      "           2       0.31      0.20      0.24      3134\n",
      "           X       0.28      0.18      0.22      3707\n",
      "\n",
      "    accuracy                           0.48     14454\n",
      "   macro avg       0.38      0.37      0.37     14454\n",
      "weighted avg       0.44      0.48      0.45     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.696901\n",
       "X    0.166390\n",
       "2    0.136710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_best = KNeighborsClassifier(n_neighbors=8, weights='distance' )\n",
    "knn_best.fit(X_train,y_train)\n",
    "y_pred_knn = knn_best.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "pd.Series(y_pred_knn).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tested a Decision Tree Classifier. Here, we can give the argument `class_weight=balanced` which gives samples of the underrepresented classes more weight in the training to combat the class imbalance. Again, a grid search is used to find the best parameter for `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_depth': np.arange(1,13)}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=20, class_weight='balanced')\n",
    "dt_cv = GridSearchCV(dt, param_grid, cv = 5)\n",
    "dt_cv.fit(X_train, y_train)\n",
    "dt_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45994188459941887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.56      0.59      7613\n",
      "           2       0.31      0.35      0.33      3134\n",
      "           X       0.30      0.34      0.32      3707\n",
      "\n",
      "    accuracy                           0.46     14454\n",
      "   macro avg       0.42      0.42      0.42     14454\n",
      "weighted avg       0.48      0.46      0.47     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.471842\n",
       "X    0.283728\n",
       "2    0.244431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=20)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "pd.Series(y_pred_dt).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is closer to the correct distribution of the classes and way better precision/recall over the KNN model in the away_win- and tie class. However, the recall for the home_win class decreased significantly and also the overall accuracy is lower.\n",
    "\n",
    "The `max_depth` of 3 also means that only 3 features are looked at when classifying a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.2222222222222222}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C' : np.linspace(0,2,10)[1:]}\n",
    "logreg = LogisticRegression(penalty=\"l1\" , class_weight='balanced', solver='liblinear')\n",
    "logreg_cv = GridSearchCV(logreg, param_grid,cv = 5)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "logreg_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5065725750657257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.76      0.66      7613\n",
      "           2       0.33      0.36      0.35      3134\n",
      "           X       0.32      0.11      0.16      3707\n",
      "\n",
      "    accuracy                           0.51     14454\n",
      "   macro avg       0.41      0.41      0.39     14454\n",
      "weighted avg       0.47      0.51      0.47     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.679604\n",
       "2    0.235921\n",
       "X    0.084475\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.222222 , penalty=\"l1\" ,class_weight='balanced', solver='liblinear')\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "pd.Series(y_pred_logreg).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model has respectable scores on the home_win and away_win classes but nearly never predicts ties, but it yields the highest accuracy out of the models presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41794658917946587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.47      0.53      7613\n",
      "           2       0.29      0.37      0.33      3134\n",
      "           X       0.28      0.34      0.31      3707\n",
      "\n",
      "    accuracy                           0.42     14454\n",
      "   macro avg       0.39      0.40      0.39     14454\n",
      "weighted avg       0.46      0.42      0.43     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.408883\n",
       "X    0.308980\n",
       "2    0.282136\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=10)\n",
    "adb = AdaBoostClassifier(base_estimator=dt , n_estimators=400)\n",
    "adb.fit(X_train, y_train)\n",
    "y_pred_adb = adb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_adb))\n",
    "print(classification_report(y_test, y_pred_adb))\n",
    "pd.Series(y_pred_adb).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the recall scores on the underrepresented classes are better than before, but the scores on the home_win class are not as good. Also, the distribution of the predicted classes is too even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49121350491213506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.71      0.64      7613\n",
      "           2       0.34      0.27      0.30      3134\n",
      "           X       0.31      0.22      0.26      3707\n",
      "\n",
      "    accuracy                           0.49     14454\n",
      "   macro avg       0.41      0.40      0.40     14454\n",
      "weighted avg       0.46      0.49      0.47     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.642729\n",
       "X    0.188391\n",
       "2    0.168881\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=8, weights='distance' )\n",
    "logreg = LogisticRegression(C=0.22222 , penalty=\"l1\" ,class_weight='balanced', solver='liblinear')\n",
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=20)\n",
    "dt_2 = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=10)\n",
    "adb = AdaBoostClassifier(base_estimator=dt_2 , n_estimators=400)\n",
    "\n",
    "classifiers = [('k nearest',knn),\n",
    "              #('logistic regression', logreg),\n",
    "              ('Decision Tree', dt),\n",
    "              ('Ada', adb)]\n",
    "\n",
    "vc = VotingClassifier(estimators = classifiers)\n",
    "vc.fit(X_train,y_train)\n",
    "y_pred_vote = vc.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_vote))\n",
    "print(classification_report(y_test, y_pred_vote))\n",
    "pd.Series(y_pred_vote).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a mix of the ones before. It retains a bit of the KNNs good scores on the home_win class, while predicting the other ones a bit more often thanks to the other models. The overall accuracy is also quite good with nearly 50% of the games predicted correctly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36061bbb730118f77daa4e36bbf3eee09c6290a0c11fb749c1a2efe69ae5881d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
