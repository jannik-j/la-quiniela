{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../laliga.sqlite\")\n",
    "df = pd.read_sql_query(\"SELECT * from Matches\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to extract the result and the goals of each match similar to the beginning of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(score: str):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    if goals[0]>goals[1]:\n",
    "        return \"1\"\n",
    "    elif goals[1]>goals[0]:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = df['score'].apply(get_result)\n",
    "df['result'] = df['result'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goals(score: str, home_away: int):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    return goals[home_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_goals'] = df['score'].apply(get_goals, args=(0,))\n",
    "df['away_goals'] = df['score'].apply(get_goals, args=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we extract the features we want to use from the data available. For a more detailed analysis of the possible impact of the features and their statistics, see the `FeatureAnalysis.ipynb` notebook.\n",
    "\n",
    "Our first features come from the matchday standings calculated in the analytical work, exercise 10. From there, we will use the rank of both of the teams and the goals scored and conceded on average up to that point of the season. We will also use the results of the last 5 games that we computed by computing a form number out of them: A win ist worth +1, a tie 0 and a loss -1. These values are then added up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_form(last_5):\n",
    "    if last_5 != \"[]\":\n",
    "        last_5 = list(last_5)\n",
    "        form = 0\n",
    "        for result in last_5:\n",
    "            if result == \"W\":\n",
    "                form += 1\n",
    "            elif result == \"T\":\n",
    "                pass\n",
    "            elif result == \"L\":\n",
    "                form -= 1\n",
    "        return form\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings = pd.read_excel('../reports/MatchdayStandings.xlsx', engine='openpyxl')\n",
    "df_standings['form'] = df_standings['last_5'].map(get_form)\n",
    "df_standings['GF'] /= df_standings['matchday']\n",
    "df_standings['GA'] /= df_standings['matchday']\n",
    "df_standings['matchday'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'home_team', 'form', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df.rename(columns={'rank': 'home_rank', 'form': 'home_form', 'GF': 'home_GF_pg', 'GA': 'home_GA_pg'}, inplace=True)\n",
    "df_standings.rename(columns={'home_team': 'away_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'away_team', 'form', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')\n",
    "df.rename(columns={'rank': 'away_rank', 'form': 'away_form', 'GF': 'away_GF_pg', 'GA': 'away_GA_pg'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to give the model more information about how well the home team was playing at home and vice versa, as a team might have a low overall rank in the standings but is near the top in the home table for example. For this we compute the home and away table similarly to the exercise 10 and extract the rank of the home team in the home table and the rank of the away team in the away table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = df.loc[df['season'] != '2021-2022'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_home_table = []\n",
    "dfs_away_table = []\n",
    "for season in df_past['season'].drop_duplicates():\n",
    "    for division in df_past.loc[(df_past['season']==season), 'division'].drop_duplicates():\n",
    "        df_games = df_past.loc[(df_past['season']==season) & (df_past['division']==division)]\n",
    "        teams = df_games['home_team'].drop_duplicates().rename('team')\n",
    "        init_data = [(season, division, 0, 0, 0, 0, 0) for _ in teams]\n",
    "        df_standings_home = pd.DataFrame(init_data, columns=['season', 'division', 'matchday', 'GF', 'GA', 'GD', 'Pts'], index=teams)\n",
    "        df_standings_away = pd.DataFrame(init_data, columns=['season', 'division', 'matchday', 'GF', 'GA', 'GD', 'Pts'], index=teams)\n",
    "        for matchday in df_games['matchday'].drop_duplicates():\n",
    "            df_standings_home['matchday'] += 1\n",
    "            df_standings_away['matchday'] += 1\n",
    "            df_matchday = df_games.loc[df_games['matchday']==matchday]            \n",
    "            for i in df_matchday.index:\n",
    "                game = df_matchday.loc[i, :]\n",
    "                df_standings_home.loc[game['home_team'], 'GF'] += game['home_goals']\n",
    "                df_standings_home.loc[game['home_team'], 'GA'] += game['away_goals']\n",
    "                df_standings_away.loc[game['away_team'], 'GF'] += game['away_goals']\n",
    "                df_standings_away.loc[game['away_team'], 'GA'] += game['home_goals']                \n",
    "                if game['result'] == '1':                    \n",
    "                    df_standings_home.loc[game['home_team'], 'Pts'] += 3                    \n",
    "                elif game['result'] == '2':                    \n",
    "                    df_standings_away.loc[game['away_team'], 'Pts'] += 3\n",
    "                else:                    \n",
    "                    df_standings_away.loc[game['home_team'], 'Pts'] += 1\n",
    "                    df_standings_away.loc[game['away_team'], 'Pts'] += 1\n",
    "                    \n",
    "            df_standings_home['GD'] = (df_standings_home['GF'] - df_standings_home['GA']).astype(int)\n",
    "            df_standings_away['GD'] = (df_standings_away['GF'] - df_standings_away['GA']).astype(int)\n",
    "\n",
    "            df_standings_home.sort_values(by=['Pts', 'GD', 'GF'], ascending=False, inplace=True)\n",
    "            df_standings_home.reset_index(inplace=True)\n",
    "            df_standings_home.insert(value=np.arange(1, len(df_standings_home)+1), loc=3, column='home_rank_HT')\n",
    "            dfs_home_table.append(df_standings_home[['season', 'division', 'matchday', 'home_rank_HT', 'team']].copy())\n",
    "            df_standings_home.drop(columns=['home_rank_HT'], inplace=True)\n",
    "            df_standings_home.set_index(keys='team', drop=True, inplace=True)\n",
    "\n",
    "            df_standings_away.sort_values(by=['Pts', 'GD', 'GF'], ascending=False, inplace=True)\n",
    "            df_standings_away.reset_index(inplace=True)\n",
    "            df_standings_away.insert(value=np.arange(1, len(df_standings_away)+1), loc=3, column='away_rank_AT')\n",
    "            dfs_away_table.append(df_standings_away[['season', 'division', 'matchday', 'away_rank_AT', 'team']].copy())\n",
    "            df_standings_away.drop(columns=['away_rank_AT'], inplace=True)\n",
    "            df_standings_away.set_index(keys='team', drop=True, inplace=True)\n",
    "\n",
    "all_home = pd.concat(dfs_home_table, ignore_index=True)\n",
    "all_home['matchday'] += 1\n",
    "all_home.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "all_away = pd.concat(dfs_away_table, ignore_index=True)\n",
    "all_away['matchday'] += 1\n",
    "all_away.rename(columns={'team': 'away_team'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(all_home, left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df = df.merge(all_away, left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our last features, we will use the results of the last 3 direct confrontations. To quantify this, we subtract the number of wins for the away team from the number of wins for the home team. (This might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_confronts(row):\n",
    "    a = row['season']\n",
    "    dt = df_past[(df_past.home_team == row.home_team) & (df_past.away_team == row.away_team)& (df_past.season < a)].tail(3)\n",
    "    res = dt[['result']].values.tolist()\n",
    "    a = res.count(['1']) - res.count(['2'])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_conf'] = df.apply(last_confronts, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will use the season as a feature because the statistics of the match outcomes have changed a lot over the years. For example, the home win rate in the first seasons was even higher than the average one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df['season'].str.split('-', n = 1, expand = True)\n",
    "df['season'] = new[0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all cells where we have missing values (e.g. the ranks on the first matchday) by zeros, because the model will not be able to predict instances where feature values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df[['home_rank','away_rank','home_form','away_form','home_GF_pg',\n",
    "              'home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']].copy()\n",
    "df_features[df_features.isna()] = 0\n",
    "df_features[['season','division','matchday','home_team','away_team','score','result']] = df[['season','division','matchday','home_team','away_team','score','result']].copy()\n",
    "df_features = df_features[['season','division','matchday','home_team','away_team','score','result','home_rank','away_rank','home_form',\n",
    "              'away_form','home_GF_pg','home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not execute all the feature extraction again when training the model, we save the file in a sqlite database. This allows us to select specific parts of the data (for example specific seasons) more easily than a csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../laliga_features.sqlite')\n",
    "df_features.to_sql('Matches', con, if_exists='replace', index=False)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most difficult aspects when training the model is the fact that our available dataset is very imbalanced. In the analytical exercises we have seen that over 52% of the games are a win for the home team. While training our first models, we saw that this led them to predict nearly always a home win. This resulted in a respectable accuracy of around 50%, but of course this is not a desired result.\n",
    "\n",
    "In this section we train a few different models and look also on the precision (what percentage of games predicted to be home/tie/away actually were correctly predicted) and recall (what percentage of games that were home/tie/away were correctly predicted) of each of the classes. In the end we want to use a `VotingClassifier` that uses the results of several different classifiers which then \"vote\" to give their final decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble  import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../laliga_features.sqlite')\n",
    "df_final = pd.read_sql_query(\"SELECT * from Matches\", con)\n",
    "con.close()\n",
    "\n",
    "df_final['result'] = pd.Categorical(df_final['result'])\n",
    "df_final = df_final[df_final['season']<2021]\n",
    "features = ['season','matchday','home_rank','away_rank','home_form','away_form','home_GF_pg',\n",
    "              'home_GA_pg','away_GF_pg','away_GA_pg','home_rank_HT','away_rank_AT','last_conf']\n",
    "X = df_final[features].copy()\n",
    "y = df_final['result'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our features have vastly different scalings, we scale them to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to test the model's behaviour on unseen data after training, we split our dataset in train- and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a k-nearest-neighbours classifier. To determine the best number of parameters, we use a grid search. The `weights=distance` option weighs the vote of the neighbours by their distance to the sample concerned, which improves the robustness to the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors' : np.arange(5,9)}\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4811816798118168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.74      0.64      7613\n",
      "           2       0.31      0.20      0.24      3134\n",
      "           X       0.28      0.18      0.22      3707\n",
      "\n",
      "    accuracy                           0.48     14454\n",
      "   macro avg       0.38      0.37      0.37     14454\n",
      "weighted avg       0.44      0.48      0.45     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.696901\n",
       "X    0.166390\n",
       "2    0.136710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_best = KNeighborsClassifier(n_neighbors=8, weights='distance' )\n",
    "knn_best.fit(X_train,y_train)\n",
    "y_pred_knn = knn_best.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "pd.Series(y_pred_knn).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tested a Decision Tree Classifier. Here, we can give the argument `class_weight=balanced` which gives samples of the underrepresented classes more weight in the training to combat the class imbalance. Again, a grid search is used to find the best parameter for `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_depth': np.arange(1,13)}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=20, class_weight='balanced')\n",
    "dt_cv = GridSearchCV(dt, param_grid, cv = 5)\n",
    "dt_cv.fit(X_train, y_train)\n",
    "dt_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45994188459941887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.56      0.59      7613\n",
      "           2       0.31      0.35      0.33      3134\n",
      "           X       0.30      0.34      0.32      3707\n",
      "\n",
      "    accuracy                           0.46     14454\n",
      "   macro avg       0.42      0.42      0.42     14454\n",
      "weighted avg       0.48      0.46      0.47     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.471842\n",
       "X    0.283728\n",
       "2    0.244431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=20)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "pd.Series(y_pred_dt).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is closer to the correct distribution of the classes and way better precision/recall over the KNN model in the away_win- and tie class. However, the recall for the home_win class decreased significantly and also the overall accuracy is lower.\n",
    "\n",
    "The `max_depth` of 3 also means that only 3 features are looked at when classifying a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41794658917946587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.47      0.53      7613\n",
      "           2       0.29      0.37      0.33      3134\n",
      "           X       0.28      0.34      0.31      3707\n",
      "\n",
      "    accuracy                           0.42     14454\n",
      "   macro avg       0.39      0.40      0.39     14454\n",
      "weighted avg       0.46      0.42      0.43     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.408883\n",
       "X    0.308980\n",
       "2    0.282136\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=10)\n",
    "adb = AdaBoostClassifier(base_estimator=dt , n_estimators=400)\n",
    "adb.fit(X_train, y_train)\n",
    "y_pred_adb = adb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_adb))\n",
    "print(classification_report(y_test, y_pred_adb))\n",
    "pd.Series(y_pred_adb).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the recall scores on the underrepresented classes are better than before, but the scores on the home_win class are not as good. Also, the distribution of the predicted classes is too even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4913518749135187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.71      0.64      7613\n",
      "           2       0.34      0.27      0.30      3134\n",
      "           X       0.31      0.23      0.26      3707\n",
      "\n",
      "    accuracy                           0.49     14454\n",
      "   macro avg       0.41      0.40      0.40     14454\n",
      "weighted avg       0.46      0.49      0.47     14454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.642729\n",
       "X    0.188322\n",
       "2    0.168950\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=8, weights='distance' )\n",
    "dt = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=20)\n",
    "dt_2 = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=10)\n",
    "adb = AdaBoostClassifier(base_estimator=dt_2 , n_estimators=400)\n",
    "\n",
    "classifiers = [('k nearest',knn),\n",
    "              ('Decision Tree', dt),\n",
    "              ('Ada', adb)]\n",
    "\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train,y_train)\n",
    "y_pred_vote = vc.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_vote))\n",
    "print(classification_report(y_test, y_pred_vote))\n",
    "pd.Series(y_pred_vote).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a mix of the ones before. It retains a bit of the KNNs good scores on the home_win class, while predicting the other ones a bit more often thanks to the other models. The overall accuracy is also quite good with nearly 50% of the games predicted correctly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36061bbb730118f77daa4e36bbf3eee09c6290a0c11fb749c1a2efe69ae5881d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
