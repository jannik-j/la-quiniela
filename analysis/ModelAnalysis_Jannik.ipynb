{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the sklearn package to implement our models, here we import the needed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"../laliga.sqlite\")\n",
    "df = pd.read_sql_query(\"SELECT * from Matches\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to extract the result of the matches from the database. We save this as \"1\" for a home win, \"2\" for a win of the away team and \"X\" for a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(score: str):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    if goals[0]>goals[1]:\n",
    "        return \"1\"\n",
    "    elif goals[1]>goals[0]:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = df['score'].apply(get_result)\n",
    "df['result'] = df['result'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the features\n",
    "### Rank and form\n",
    "\n",
    "As features to train the model we want to use different values from the standings, most of them were already computed in the exercises of the analytical work.\n",
    "\n",
    "The most obvious one is the rank of the two teams before the match. We make this rank relative, meaning we divide it by the number of teams in the respective division in the same season to make it comparable over the seasons.\n",
    "\n",
    "We also want to use the results of the last 5 matches, which we convert into a numerical value by assigning the values +1, 0, -1 to a win, tie and a loss and summing them up. We'll call this value the \"form\" of the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_form(last_5):\n",
    "    if last_5 != \"[]\":\n",
    "        last_5 = list(last_5)\n",
    "        form = 0\n",
    "        for result in last_5:\n",
    "            if result == \"W\":\n",
    "                form += 1\n",
    "            elif result == \"T\":\n",
    "                pass\n",
    "            elif result == \"L\":\n",
    "                form -= 1\n",
    "        return form\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_rank(df_standings):\n",
    "    for season in df_standings.season.drop_duplicates():\n",
    "        num_teams = df_standings.loc[df_standings['season']==season, 'team'].drop_duplicates().count()\n",
    "        df_standings.loc[df_standings['season']==season, 'rank'] /= num_teams\n",
    "    return df_standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>division</th>\n",
       "      <th>matchday</th>\n",
       "      <th>rank</th>\n",
       "      <th>team</th>\n",
       "      <th>GF</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>T</th>\n",
       "      <th>Pts</th>\n",
       "      <th>last_5</th>\n",
       "      <th>form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1928-1929</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Real Madrid</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['W']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1928-1929</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['W']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1928-1929</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Espanyol</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['W']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1928-1929</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Athletic Madrid</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['W']</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1928-1929</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Donostia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['T']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  division  matchday  rank             team   GF   GA  GD  W  L  \\\n",
       "0  1928-1929         1         2   0.1      Real Madrid  5.0  0.0   5  1  0   \n",
       "1  1928-1929         1         2   0.2        Barcelona  2.0  0.0   2  1  0   \n",
       "2  1928-1929         1         2   0.3         Espanyol  3.0  2.0   1  1  0   \n",
       "3  1928-1929         1         2   0.4  Athletic Madrid  3.0  2.0   1  1  0   \n",
       "4  1928-1929         1         2   0.5         Donostia  1.0  1.0   0  0  0   \n",
       "\n",
       "   T  Pts last_5  form  \n",
       "0  0    3  ['W']   1.0  \n",
       "1  0    3  ['W']   1.0  \n",
       "2  0    3  ['W']   1.0  \n",
       "3  0    3  ['W']   1.0  \n",
       "4  1    1  ['T']   0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_standings = pd.read_excel('../reports/MatchdayStandings.xlsx', engine='openpyxl')\n",
    "df_standings['form'] = df_standings['last_5'].map(get_form)\n",
    "df_standings = calc_relative_rank(df_standings)\n",
    "df_standings['GF'] /= df_standings['matchday']\n",
    "df_standings['GA'] /= df_standings['matchday']\n",
    "df_standings['matchday'] += 1\n",
    "df_standings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'home_team', 'form']], left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df.rename(columns={'rank': 'rank_home', 'form': 'form_home'}, inplace=True)\n",
    "df_standings.rename(columns={'home_team': 'away_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'rank', 'away_team', 'form']], left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')\n",
    "df.rename(columns={'rank': 'rank_away', 'form': 'form_away'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'division', 'matchday', 'date', 'time', 'home_team',\n",
       "       'away_team', 'score', 'result', 'rank_home', 'form_home', 'rank_away',\n",
       "       'form_away'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals scored and conceded\n",
    "We also want to include the average goals scored and conceded by each team in the season up until the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings.rename(columns={'team': 'home_team', 'away_team': 'home_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'home_team', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df.rename(columns={'GF': 'GF_home', 'GA': 'GA_home'}, inplace=True)\n",
    "df_standings.rename(columns={'home_team': 'away_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'away_team', 'GF', 'GA']], left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')\n",
    "df.rename(columns={'GF': 'GF_away', 'GA': 'GA_away'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is using only the average goals scored and conceded in the last 5 matches so that the form of the teams in the last matches is more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings = pd.read_excel('../reports/MatchdayStandings.xlsx', engine='openpyxl')\n",
    "df_standings['form'] = df_standings['last_5'].map(get_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goals(score: str, home_away: int):\n",
    "    if score is None:\n",
    "        return None\n",
    "    goals = list(map(int, score.split(':')))\n",
    "    return goals[home_away]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_goals'] = df['score'].apply(get_goals, args=(0,))\n",
    "df['away_goals'] = df['score'].apply(get_goals, args=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = df[df['season']!='2021-2022'].copy()\n",
    "dfs = []\n",
    "for season in df_past['season'].drop_duplicates():\n",
    "    for division in df_past.loc[(df_past['season']==season), 'division'].drop_duplicates():\n",
    "        df_games = df_past.loc[(df_past['season']==season) & (df_past['division']==division)]\n",
    "        team = df_games['home_team'].drop_duplicates().rename('team')\n",
    "        init_data = [([], []) for _ in team]\n",
    "        df_goals = pd.DataFrame(init_data, columns=['last_5_goals_scored', 'last_5_goals_conceded'], index=team)\n",
    "        last_5_goals_scored = df_goals['last_5_goals_scored'].copy()\n",
    "        last_5_goals_conceded = df_goals['last_5_goals_conceded'].copy()\n",
    "        for matchday in df_games['matchday'].drop_duplicates():\n",
    "            df_standings_matchday = df_standings.loc[(df_standings['season']==season) & (df_standings['division']==division) & (df_standings['matchday']==matchday)].copy()\n",
    "            df_matchday = df_games.loc[df_games['matchday']==matchday]\n",
    "            last_5_goals_scored = df_goals['last_5_goals_scored'].apply(lambda x: x[:4])\n",
    "            last_5_goals_conceded = df_goals['last_5_goals_conceded'].apply(lambda x: x[:4])\n",
    "            for i in df_matchday.index:\n",
    "                game = df_matchday.loc[i, :]\n",
    "                last_5_goals_scored.loc[game['home_team']] = [game['home_goals']] + last_5_goals_scored.loc[game['home_team']]\n",
    "                last_5_goals_scored.loc[game['away_team']] = [game['away_goals']] + last_5_goals_scored.loc[game['away_team']]\n",
    "                last_5_goals_conceded.loc[game['home_team']] = [game['away_goals']] + last_5_goals_conceded.loc[game['home_team']]\n",
    "                last_5_goals_conceded.loc[game['away_team']] = [game['home_goals']] + last_5_goals_conceded.loc[game['away_team']]\n",
    "            df_goals['last_5_goals_scored'] = last_5_goals_scored\n",
    "            df_goals['last_5_goals_conceded'] = last_5_goals_conceded\n",
    "            df_goals.reset_index(drop=False, inplace=True)\n",
    "            df_standings_matchday = df_standings_matchday.merge(df_goals, left_on='team', right_on='team', how='left')\n",
    "            dfs.append(df_standings_matchday)\n",
    "            df_goals = df_goals.set_index('team', drop=True)\n",
    "df_standings = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standings['GF'] /= df_standings['matchday']\n",
    "df_standings['GA'] /= df_standings['matchday']\n",
    "df_standings['matchday'] += 1\n",
    "df_standings.rename(columns={'team': 'home_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'home_team', 'last_5_goals_scored', 'last_5_goals_conceded']], left_on=['season', 'division', 'matchday', 'home_team'], right_on=['season', 'division', 'matchday', 'home_team'], how='left')\n",
    "df.rename(columns={'last_5_goals_scored': 'l5_goals_scored_home', 'last_5_goals_conceded': 'l5_goals_conceded_home'}, inplace=True)\n",
    "df_standings.rename(columns={'home_team': 'away_team'}, inplace=True)\n",
    "df = df.merge(df_standings[['season', 'division', 'matchday', 'away_team', 'last_5_goals_scored', 'last_5_goals_conceded']], left_on=['season', 'division', 'matchday', 'away_team'], right_on=['season', 'division', 'matchday', 'away_team'], how='left')\n",
    "df.rename(columns={'last_5_goals_scored': 'l5_goals_scored_away', 'last_5_goals_conceded': 'l5_goals_conceded_away'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_last_goals(goals_list):\n",
    "    if isinstance(goals_list, list):\n",
    "        if len(goals_list)==0:\n",
    "            return 0\n",
    "        else:\n",
    "            return sum(goals_list)/len(goals_list)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['l5_goals_scored_home', 'l5_goals_conceded_home', 'l5_goals_scored_away', 'l5_goals_conceded_away']] = df[['l5_goals_scored_home', 'l5_goals_conceded_home', 'l5_goals_scored_away', 'l5_goals_conceded_away']].applymap(sum_last_goals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of last season\n",
    "The features computed so far have no values for the first matchday of each season. To cope with that, we also introduce the rank of each team in the previous season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_season_standings = pd.read_excel('../reports/SeasonStandings.xlsx', engine='openpyxl')\n",
    "seasons = df['season'].drop_duplicates().copy().to_numpy()\n",
    "df['last_rank_home'] = [0 for _ in df.index]\n",
    "df['last_rank_away'] = [0 for _ in df.index]\n",
    "for i in np.arange(len(seasons)-1):\n",
    "    df_this_season = df_season_standings.loc[df_season_standings['season']==seasons[i],:]\n",
    "    teams = df_this_season['team'].drop_duplicates().copy()\n",
    "    for team in teams:\n",
    "        rank_team = ((df_this_season.loc[(df_this_season['team']==team), 'division'])*df_this_season.loc[(df_this_season['team']==team), 'rank']).to_numpy()[0]\n",
    "        df.loc[(df['season']==seasons[i+1]) & (df['home_team']==team), 'last_rank_home'] = rank_team\n",
    "        df.loc[(df['season']==seasons[i+1]) & (df['away_team']==team), 'last_rank_away'] = rank_team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the differences of the features\n",
    "To reduce the amount of features we will try to train the model not with the features for each team but the differences of the features between the teams. Intuitively this should retain most of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank_diff'] = df['rank_home'] - df['rank_away']\n",
    "df['form_diff'] = df['form_home'] - df['form_away']\n",
    "df['GF_diff'] = df['GF_home'] - df['GF_away']\n",
    "df['GA_diff'] = df['GA_home'] - df['GA_away']\n",
    "df['l5_goals_scored_diff'] = df['l5_goals_scored_home'] - df['l5_goals_scored_away']\n",
    "df['l5_goals_conceded_diff'] = df['l5_goals_conceded_home'] - df['l5_goals_conceded_away']\n",
    "df['last_rank_diff'] = df['last_rank_home'] - df['last_rank_away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "### GradientBoostedClassifier\n",
    "As in the lectures, we first experiment with a gradient-boosted model to classify the matches. For that we introduce a function that fills NaN values in the data with zeros to use more data points, does a train/test split of the data, trains the model and returns the it as well as the predictions on the test set. We also introduce a function to compute the accuracy of the model, that is, the percentage of correctly predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(y_test, y_pred):\n",
    "    return sum(y_test==y_pred)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbm(features, target, gbm_hyperparams):\n",
    "    df_past = df.loc[df['season']!='2021-2022']\n",
    "    X = df_past[features].copy()\n",
    "    y = df_past[target].copy()\n",
    "    X[X.isna()] = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    gbm_model = GradientBoostingClassifier(**gbm_hyperparams)\n",
    "    gbm_model.fit(X_train, y_train)\n",
    "    gbm_y_pred = gbm_model.predict(X_test)\n",
    "    results_df = X_test.copy()\n",
    "    results_df[\"y_real\"] = y_test\n",
    "    results_df[\"y_pred\"] = gbm_y_pred\n",
    "    print(f\"Model accuracy: {model_accuracy(results_df['y_real'], results_df['y_pred'])}\")\n",
    "    return gbm_model, results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train a model on all the features we computed in the first section except for the goals scored and conceded in the last 5 matches. After testing some different learning rates, we saw that a smaller learning rate than the default 0.1 yields much better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5321710253217102\n"
     ]
    }
   ],
   "source": [
    "gbm_hyperparams = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'loss': 'deviance'\n",
    "}\n",
    "features = ['rank_home', 'form_home', 'rank_away', 'form_away', 'GF_home', 'GA_home', 'GF_away', 'GA_away', 'last_rank_home', 'last_rank_away']\n",
    "target = 'result'\n",
    "model_all_features, results_all_features = train_gbm(features, target, gbm_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on this model already looks quite good. However we see a big problem, when we check the distribution of the predicted values: It nearly only predicts a home win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9304\n",
       "2     261\n",
       "X      71\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all_features['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is caused by the class imbalance: As we have seen in the analysis exercises, about 52% of the games are home wins. To cope with that, we use the `imbalanced_learn` library. From there we use the `SMOTE` class, which implements the Synthetic Minority Over-sampling Technique to create artificial samples of the minority classes to balance out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbm(features, target, gbm_hyperparams):\n",
    "    sm = SMOTE()\n",
    "    df_past = df.loc[df['season']!='2021-2022']\n",
    "    X = df_past[features].copy()\n",
    "    y = df_past[target].copy()\n",
    "    X[X.isna()] = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    gbm_model = GradientBoostingClassifier(**gbm_hyperparams)\n",
    "    gbm_model.fit(X_train, y_train)\n",
    "    gbm_y_pred = gbm_model.predict(X_test)\n",
    "    results_df = X_test.copy()\n",
    "    results_df[\"y_real\"] = y_test\n",
    "    results_df[\"y_pred\"] = gbm_y_pred\n",
    "    print(f\"Model accuracy: {model_accuracy(results_df['y_real'], results_df['y_pred'])}\")\n",
    "    return gbm_model, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.4950186799501868\n"
     ]
    }
   ],
   "source": [
    "gbm_hyperparams = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance'\n",
    "}\n",
    "features = ['rank_home', 'form_home', 'rank_away', 'form_away', 'GF_home', 'GA_home', 'GF_away', 'GA_away', 'last_rank_home', 'last_rank_away']\n",
    "target = 'result'\n",
    "model_all_features_smote, results_all_features_smote = train_gbm(features, target, gbm_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6136\n",
       "X    1781\n",
       "2    1719\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all_features_smote['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this makes the model predict a tie or an away win more often while preserving most of the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to test some different combinations of the features. First we will drop the overall goals scored and conceded and consider the goals scored and conceded only in the last 5 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5158779576587795\n"
     ]
    }
   ],
   "source": [
    "gbm_hyperparams = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance'\n",
    "}\n",
    "features = ['rank_home', 'form_home', 'rank_away', 'form_away', 'l5_goals_scored_home', 'l5_goals_conceded_home', 'l5_goals_scored_away', 'l5_goals_conceded_away', 'last_rank_home', 'last_rank_away']\n",
    "target = 'result'\n",
    "model_all_features_l5goals, results_all_features_l5goals = train_gbm(features, target, gbm_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model performance is comparable. Because the overall goals scored and conceded requires much less computation time (as we can just extract it from our standings table), we will use them.\n",
    "\n",
    "The next thing we check is how the model performs when we use only the differences in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.47063096720630965\n"
     ]
    }
   ],
   "source": [
    "gbm_hyperparams = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance'\n",
    "}\n",
    "features = ['rank_diff', 'form_diff', 'GF_diff', 'GA_diff', 'last_rank_diff', 'matchday']\n",
    "target = 'result'\n",
    "model_diffs, results_diffs = train_gbm(features, target, gbm_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also this model has a similar performance to the other ones, but we managed to achieve this with half of the features. Experimenting with the hyper-parameters gives the insight that a higher learning rate biases the model more towards a home-win but increases the overall accuracy. This trade-off has to be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still predicts mostly home wins, but not as often as our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.592258\n",
       "X    0.213263\n",
       "2    0.194479\n",
       "Name: y_pred, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_diffs['y_pred'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65381589, 0.15224233, 0.19394178],\n",
       "       [0.4929312 , 0.2869934 , 0.2200754 ],\n",
       "       [0.55020576, 0.20205761, 0.24773663]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(results_diffs['y_real'], results_diffs['y_pred'], normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.65      0.62      5084\n",
      "           2       0.32      0.29      0.30      2122\n",
      "           X       0.29      0.25      0.27      2430\n",
      "\n",
      "    accuracy                           0.47      9636\n",
      "   macro avg       0.40      0.40      0.40      9636\n",
      "weighted avg       0.45      0.47      0.46      9636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(results_diffs['y_real'], results_diffs['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the confusion matrix, that still about 50% of the away wins and 58% of the ties are wrongly classified as home wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank_diff         0.224007\n",
       "GF_diff           0.188819\n",
       "form_diff         0.172966\n",
       "GA_diff           0.168920\n",
       "last_rank_diff    0.133766\n",
       "matchday          0.111522\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.Series(model_diffs.feature_importances_, index=features)\n",
    "importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.43295973432959733\n"
     ]
    }
   ],
   "source": [
    "rforest_hyperparams = {\n",
    "    'n_estimators': 500,\n",
    "    'criterion': 'gini',\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "#features = ['rank_diff', 'form_diff', 'l5_goals_scored_diff', 'l5_goals_conceded_diff', 'GF_diff', 'GA_diff']\n",
    "features = ['rank_diff', 'form_diff', 'GF_diff', 'GA_diff', 'last_rank_diff']\n",
    "target = 'result'\n",
    "df_past = df.loc[df['season']!='2021-2022']\n",
    "X = df_past[features].copy()\n",
    "y = df_past[target].copy()\n",
    "X[X.isna()] = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "sm = SMOTE()\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "rforest_model_diff = RandomForestClassifier(**rforest_hyperparams)\n",
    "rforest_model_diff.fit(X_train, y_train)\n",
    "rforest_y_pred_diff = rforest_model_diff.predict(X_test)\n",
    "\n",
    "results_rforest_diff = X_test.copy()\n",
    "results_rforest_diff[\"y_real\"] = y_test\n",
    "results_rforest_diff[\"y_pred\"] = rforest_y_pred_diff\n",
    "print(f\"Model accuracy: {model_accuracy(results_rforest_diff['y_real'], results_rforest_diff['y_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.518161\n",
       "X    0.242217\n",
       "2    0.239622\n",
       "Name: y_pred, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rforest_diff['y_pred'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest-Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12964, 5)\n",
      "Model accuracy: 0.39092872570194387\n"
     ]
    }
   ],
   "source": [
    "knn_hyperparams = {\n",
    "    'n_neighbors': 5,\n",
    "    'weights': 'distance',\n",
    "    'algorithm': 'auto',\n",
    "    'metric': 'minkowski',\n",
    "    'p': 2\n",
    "}\n",
    "sm = SMOTE()\n",
    "#features = ['rank_diff', 'form_diff', 'l5_goals_scored_diff', 'l5_goals_conceded_diff', 'GF_diff', 'GA_diff']\n",
    "features = ['rank_diff', 'form_diff', 'GF_diff', 'GA_diff', 'last_rank_diff']\n",
    "#features = ['rank_home', 'form_home', 'rank_away', 'form_away', 'GF_home', 'GA_home', 'GF_away', 'GA_away', 'last_rank_home', 'last_rank_away']\n",
    "target = 'result'\n",
    "df_past = df.loc[df['season']!='2021-2022'].copy()\n",
    "df_past.dropna(inplace=True)\n",
    "X = df_past[features]\n",
    "y = df_past[target]\n",
    "#X[X.isna()] = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "knn_model = KNeighborsClassifier(**knn_hyperparams)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_y_pred = knn_model.predict(X_test)\n",
    "\n",
    "results_df_knn = X_test.copy()\n",
    "results_df_knn[\"y_real\"] = y_test\n",
    "results_df_knn[\"y_pred\"] = knn_y_pred\n",
    "print(f\"Model accuracy: {model_accuracy(results_df_knn['y_real'], results_df_knn['y_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.520210\n",
       "X    0.251774\n",
       "2    0.228016\n",
       "Name: y_pred, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_knn['y_pred'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db7cb45de2fae031819628b6083a7f9eedb36f337bc3654294b4af92d26dacc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
